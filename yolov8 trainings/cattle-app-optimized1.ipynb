{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8847962,"sourceType":"datasetVersion","datasetId":5325617},{"sourceId":8875949,"sourceType":"datasetVersion","datasetId":5342895}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-06T06:25:38.728215Z","iopub.execute_input":"2024-07-06T06:25:38.728668Z","iopub.status.idle":"2024-07-06T06:25:57.001956Z","shell.execute_reply.started":"2024-07-06T06:25:38.728631Z","shell.execute_reply":"2024-07-06T06:25:57.000596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n# Load YOLOv5 models\n\n# from utils.general import non_max_suppression, scale_coords\n# from utils.torch_utils import select_device, time_sync\n\n# Define paths to your models\n\n\ndef adjust(segImg):\n    # Adjust the segmentation image as needed\n    im_width = segImg.width\n    im_height = segImg.height\n    upper_px = ()\n    lower_px = ()\n    \n    # Example adjustment logic (modify as per your specific needs)\n    for i in range(im_height):\n        flag = 0\n        for j in range(im_width):\n            coordinate = j, i\n            if segImg.getpixel(coordinate) == 1:\n                upper_px = (j, i)\n                flag = 1\n                break\n        \n        if flag == 1:\n            break\n    \n    for i in reversed(range(im_height)):\n        flag = 0\n        for j in range(im_width):\n            coordinate = j, i\n            if segImg.getpixel(coordinate) == 1:\n                lower_px = (j, i)\n                flag = 1\n                break\n        \n        if flag == 1:\n            break\n    \n    return upper_px, lower_px\n\ndef preprocess_image(image_path):\n    # Read and preprocess the image\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\ndef segment_image(image):\n    # Perform segmentation using YOLOv5 model (example, adjust as needed)\n    imgsz = 640  # size of the input image\n#     stride = int(seg_model.stride.max())  # model stride\n    img = image.copy()\n\n    # Inference\n    img = torch.from_numpy(img)\n    img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0\n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = seg_model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n\n    # Process results\n    for i, det in enumerate(pred):  # detections per image\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], image.shape).round()\n    \n    return pred\n\ndef detect_keypoints_side(image):\n    # Perform side keypoint detection using YOLOv5 model (example, adjust as needed)\n    imgsz = 640  # size of the input image\n    stride = int(pose_side_model.stride.max())  # model stride\n    img = image.copy()\n\n    # Inference\n    img = torch.from_numpy(img)\n    img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0\n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = pose_side_model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n\n    # Process results\n    for i, det in enumerate(pred):  # detections per image\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], image.shape).round()\n\n    return pred\n\ndef detect_keypoints_rear(image):\n    # Perform rear keypoint detection using YOLOv5 model (example, adjust as needed)\n    imgsz = 640  # size of the input image\n    stride = int(pose_rear_model.stride.max())  # model stride\n    img = image.copy()\n\n    # Inference\n    img = torch.from_numpy(img)\n    img = img.float() / 255.0  # 0 - 255 to 0.0 - 1.0\n    if img.ndimension() == 3:\n        img = img.unsqueeze(0)\n\n    # Inference\n    pred = pose_rear_model(img, augment=False)[0]\n\n    # Apply NMS\n    pred = non_max_suppression(pred, conf_thres=0.25, iou_thres=0.45)\n\n    # Process results\n    for i, det in enumerate(pred):  # detections per image\n        if det is not None and len(det):\n            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], image.shape).round()\n\n    return pred\n\ndef draw_masks(image, masks):\n    # Draw segmentation masks on the image\n    # Example drawing logic (modify as per your specific needs)\n    for mask in masks:\n        mask = (mask * 255).astype(np.uint8)\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n    return image\n\ndef draw_keypoints(image, keypoints, color=(0, 0, 255)):\n    # Draw keypoints on the image\n    # Example drawing logic (modify as per your specific needs)\n    for kp_set in keypoints:\n        for kp in kp_set:\n            x, y = kp[:2]\n            cv2.circle(image, (int(x), int(y)), 5, color, -1)\n    return image\n\ndef infer(image_path):\n    # Load and preprocess the image\n    image = preprocess_image(image_path)\n\n    # Perform segmentation\n    masks = segment_image(image)\n    image_with_masks = draw_masks(image.copy(), masks)\n\n    # Perform side keypoint detection\n    side_keypoints = detect_keypoints_side(image)\n    image_with_side_keypoints = draw_keypoints(image_with_masks.copy(), side_keypoints, color=(255, 0, 0))\n\n    # Perform rear keypoint detection\n    rear_keypoints = detect_keypoints_rear(image)\n    final_image = draw_keypoints(image_with_side_keypoints.copy(), rear_keypoints, color=(0, 0, 255))\n\n    return final_image\n\n# Example usage\nimage_path = '/kaggle/input/cow-side-back/1.0_r_117_10.0_F.jpg'\noutput_image = infer(image_path)\ncv2.imshow('Output Image', output_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:30:41.998866Z","iopub.execute_input":"2024-07-06T06:30:41.999350Z","iopub.status.idle":"2024-07-06T06:30:45.907096Z","shell.execute_reply.started":"2024-07-06T06:30:41.999313Z","shell.execute_reply":"2024-07-06T06:30:45.905104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_yolov8_pose(pose_model, img):\n    # Perform pose estimation using the pose model\n    results = pose_model.predict(img)\n    keypoints = []\n    for result in results:\n        keypoints.append({\n            \"keypoints\": np.array(result[\"keypoints\"])\n        })\n    return keypoints\n\ndef inference_yolov8_seg(seg_model, img):\n    # Perform segmentation using the segmentation model\n    results = seg_model.predict(img)\n    segmentation_masks = []\n    for result in results:\n        segmentation_masks.append(np.array(result[\"mask\"]))\n    return segmentation_masks","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:54:48.213776Z","iopub.execute_input":"2024-07-06T06:54:48.214291Z","iopub.status.idle":"2024-07-06T06:54:48.223717Z","shell.execute_reply.started":"2024-07-06T06:54:48.214257Z","shell.execute_reply":"2024-07-06T06:54:48.222135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport joblib\nfrom PIL import Image\nimport torch\nfrom ultralytics import YOLO\n\n# Assuming YOLOv8 models have similar init and inference functions\n\ndef adjust(segImg):\n    im_width, im_height = segImg.size\n    upper_px = lower_px = None\n\n    for i in range(im_height):\n        for j in range(im_width):\n            if segImg.getpixel((j, i)) == 1:\n                upper_px = (j, i)\n                break\n        if upper_px:\n            break\n\n    for i in reversed(range(im_height)):\n        for j in range(im_width):\n            if segImg.getpixel((j, i)) == 1:\n                lower_px = (j, i)\n                break\n        if lower_px:\n            break\n\n    return upper_px, lower_px\n\ndef y_distance(point1, point2):\n    return round(abs(point1[1] - point2[1]))\n\ndef load_models():\n    path_to_seg_yolov8 = '/kaggle/input/cattle-4-models/Final 4 model weights/best_seg_yolov8l.pt'\n    path_to_pose_side_yolo = '/kaggle/input/cattle-4-models/Final 4 model weights/best_pose_side_yolov8l.pt'\n    path_to_pose_rear_yolo = '/kaggle/input/cattle-4-models/Final 4 model weights/best_pose_rear_yolov8l.pt'\n    weight_filename = '/kaggle/input/cattle-4-models/Final 4 model weights/model_v3.joblib'\n\n    seg_model = YOLO(path_to_seg_yolov8)\n    pose_side_model = YOLO(path_to_pose_side_yolo)\n    pose_rear_model = YOLO(path_to_pose_rear_yolo)\n\n    weight_model = joblib.load(weight_filename)\n\n    return seg_model, pose_rear_model, pose_side_model, weight_model\n\ndef process_image(det_model, seg_model, pose_model, img):\n    # Perform segmentation inference\n    seg_results = inference_yolov8_seg(seg_model, img)\n    \n    # Perform pose estimation inference\n    pose_results = inference_yolov8_pose(pose_model, img)\n    \n    return pose_results, seg_results\n\ndef calculate_features(rear_pose_results, side_pose_results, side_seg_result, rear_seg_result):\n    rear_kpt = rear_pose_results[0][\"keypoints\"][:, :2]\n    side_kpt = side_pose_results[0][\"keypoints\"][:, :2]\n\n    if side_kpt.shape != (9, 2) or rear_kpt.shape != (4, 2):\n        raise ValueError(\"Incorrect keypoint shape\")\n\n    segImg = Image.fromarray(np.array(side_seg_result[0].astype('uint8')))\n    segRear = Image.fromarray(np.array(rear_seg_result[0].astype('uint8')))\n    rear_p1, rear_p2 = adjust(segRear)\n    rear_height = y_distance(rear_p1, rear_p2)\n\n    side_im_width, side_im_height = segImg.size\n    if int(side_kpt[1, 0]) < (side_im_width / 2):\n        seg_crop = segImg.crop((0, 0, int(side_kpt[8, 0]), side_im_height))\n    else:\n        seg_crop = segImg.crop((int(side_kpt[8, 0]), 0, side_im_width, side_im_height))\n\n    side_p1, side_p2 = adjust(seg_crop)\n    side_height = y_distance(side_p1, side_p2)\n    side_Length_shoulderbone = round(np.linalg.norm(side_kpt[2] - side_kpt[1]))\n    side_F_Girth = round(np.linalg.norm(side_kpt[4] - side_kpt[3]))\n    side_R_Girth = round(np.linalg.norm(side_kpt[8] - side_kpt[7]))\n    rear_width = round(np.linalg.norm(rear_kpt[1] - rear_kpt[0]))\n    actual_width = rear_width * (side_height / rear_height)\n\n    return side_Length_shoulderbone, side_F_Girth, side_R_Girth, actual_width\n\ndef predict_weight(weight_model, features, sticker, cattle):\n    predicted_cattle_weight = float(weight_model.predict([features]))\n    ratio = cattle / sticker\n\n    adjustment_factors = [\n        (50, 0.68), (55, 0.57), (60, 0.48), (65, 0.40), (67, 0.28),\n        (70, 0), (72, 0.25), (75, 0.35), (80, 0.45), (85, 0.55),\n        (90, 0.65), (95, 0.75), (100, 0.85), (105, 0.95), (110, 1.05),\n        (115, 1.15), (120, 1.25), (125, 1.35), (130, 1.45), (135, 1.55),\n        (140, 1.65), (145, 1.75), (150, 1.95), (155, 2.05), (160, 2.20),\n        (165, 2.35), (170, 2.45), (180, 2.55), (190, 2.65), (200, 2.75),\n        (210, 2.85), (220, 2.95), (230, 3.05)\n    ]\n\n    for threshold, factor in adjustment_factors:\n        if ratio < threshold:\n            predicted_cattle_weight -= ratio * factor\n            break\n    else:\n        predicted_cattle_weight = 0\n        status = \"Sorry! This cattle can't be handled right now.\"\n        return {\"weight\": predicted_cattle_weight, \"ratio\": ratio, \"remarks\": status}\n\n    return {\"weight\": predicted_cattle_weight, \"ratio\": ratio, \"remarks\": \"ok\"}\n\ndef predict(side_fname, rear_fname):\n    print(torch.__version__, torch.cuda.is_available())\n\n    try:\n        seg_model, rear_pose_model, side_pose_model, weight_model = load_models()\n        rear_pose_results, rear_seg_result = process_image(None, seg_model, rear_pose_model, rear_fname)\n        side_pose_results, side_seg_result = process_image(None, seg_model, side_pose_model, side_fname)\n\n        seg = np.asarray(side_seg_result)\n        sticker = (seg == 2).sum()\n        cattle = (seg == 1).sum()\n\n        if sticker < 100:\n            return {\"weight\": 0, \"ratio\": cattle / sticker, \"remarks\": \"Please apply sticker correctly.\"}\n\n        features = calculate_features(rear_pose_results, side_pose_results, side_seg_result, rear_seg_result)\n        return predict_weight(weight_model, features, sticker, cattle)\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {\"weight\": 0, \"ratio\": 0, \"remarks\": \"An error occurred during processing.\"}\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:54:50.241625Z","iopub.execute_input":"2024-07-06T06:54:50.242080Z","iopub.status.idle":"2024-07-06T06:54:50.294678Z","shell.execute_reply.started":"2024-07-06T06:54:50.242019Z","shell.execute_reply":"2024-07-06T06:54:50.292508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict('/kaggle/input/cow-side-back/10.0_s_141_2.0_M.jpg','/kaggle/input/cow-side-back/10.0_r_141_2.0_M.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:54:54.206725Z","iopub.execute_input":"2024-07-06T06:54:54.207151Z","iopub.status.idle":"2024-07-06T06:55:01.496979Z","shell.execute_reply.started":"2024-07-06T06:54:54.207118Z","shell.execute_reply":"2024-07-06T06:55:01.494903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef inference_yolov8_pose(pose_model, img):\n    results = pose_model.predict(img)\n    keypoints = []\n    for result in results:\n        keypoints.append({\n            \"keypoints\": np.array(result.keypoints.cpu())  # Ensure keypoints are on CPU\n        })\n    return keypoints\n\ndef inference_yolov8_seg(seg_model, img):\n    results = seg_model.predict(img)\n    segmentation_masks = []\n    for result in results:\n        segmentation_masks.append(np.array(result.masks.cpu()))  # Ensure masks are on CPU\n    return segmentation_masks\n\ndef process_image(pose_model, seg_model, img):\n    seg_results = inference_yolov8_seg(seg_model, img)\n    pose_results = inference_yolov8_pose(pose_model, img)\n    return pose_results, seg_results\n\ndef predict(side_fname, rear_fname):\n    print(torch.__version__, torch.cuda.is_available())\n\n    try:\n        seg_model, rear_pose_model, side_pose_model, weight_model = load_models()\n        rear_pose_results, rear_seg_result = process_image(rear_pose_model, seg_model, rear_fname)\n        side_pose_results, side_seg_result = process_image(side_pose_model, seg_model, side_fname)\n\n        # Debugging: print shapes and types of results\n        print(f\"rear_pose_results: {rear_pose_results}\")\n        print(f\"rear_seg_result: {rear_seg_result}\")\n        print(f\"side_pose_results: {side_pose_results}\")\n        print(f\"side_seg_result: {side_seg_result}\")\n\n        seg = np.asarray(side_seg_result[0])\n        print(f\"seg shape: {seg.shape}, seg dtype: {seg.dtype}\")\n        \n        sticker = (seg == 2).sum()\n        cattle = (seg == 1).sum()\n\n        print(f\"sticker count: {sticker}, cattle count: {cattle}\")\n\n        if sticker < 100:\n            return {\"weight\": 0, \"ratio\": cattle / sticker, \"remarks\": \"Please apply sticker correctly.\"}\n\n        features = calculate_features(rear_pose_results, side_pose_results, side_seg_result, rear_seg_result)\n        return predict_weight(weight_model, features, sticker, cattle)\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {\"weight\": 0, \"ratio\": 0, \"remarks\": \"An error occurred during processing.\"}\n","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:56:28.565391Z","iopub.execute_input":"2024-07-06T06:56:28.566780Z","iopub.status.idle":"2024-07-06T06:56:28.587578Z","shell.execute_reply.started":"2024-07-06T06:56:28.566728Z","shell.execute_reply":"2024-07-06T06:56:28.586021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict('/kaggle/input/cow-side-back/10.0_s_141_2.0_M.jpg','/kaggle/input/cow-side-back/10.0_r_141_2.0_M.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:56:41.378086Z","iopub.execute_input":"2024-07-06T06:56:41.378487Z","iopub.status.idle":"2024-07-06T06:56:45.900087Z","shell.execute_reply.started":"2024-07-06T06:56:41.378456Z","shell.execute_reply":"2024-07-06T06:56:45.898823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_yolov8_pose(pose_model, img):\n    # Perform pose estimation using the pose model\n    results = pose_model.predict(img)\n    keypoints = []\n    for result in results:\n        keypoints.append({\n            \"keypoints\": np.array(result.keypoints.data.cpu(), dtype=np.float32)  # Ensure keypoints are on CPU and float32\n        })\n    return keypoints\n\ndef inference_yolov8_seg(seg_model, img):\n    # Perform segmentation using the segmentation model\n    results = seg_model.predict(img)\n    segmentation_masks = []\n    for result in results:\n        segmentation_masks.append(np.array(result.masks.data.cpu(), dtype=np.uint8))  # Ensure masks are on CPU and uint8\n    return segmentation_masks\ndef process_image(pose_model, seg_model, img):\n    seg_results = inference_yolov8_seg(seg_model, img)\n    pose_results = inference_yolov8_pose(pose_model, img)\n    return pose_results, seg_results\n\ndef calculate_features(rear_pose_results, side_pose_results, side_seg_result, rear_seg_result, sticker, cattle):\n    \n    rear_kpt = rear_pose_results[0][\"keypoints\"][0][:, :2]\n    side_kpt = side_pose_results[0][\"keypoints\"][0][:, :2]\n    \n\n    if side_kpt.shape != (9, 2) or rear_kpt.shape != (4, 2):\n        raise ValueError(\"Incorrect keypoint shape\")\n        \n    segImg = Image.fromarray(np.array(side_seg_result[0][0].astype('uint8')))\n    segRear = Image.fromarray(np.array(rear_seg_result[0][0].astype('uint8')))\n    rear_p1, rear_p2 = adjust(segRear)\n    print(rear_p1, rear_p2)\n    rear_height = y_distance(rear_p1, rear_p2)\n\n    side_im_width, side_im_height = segImg.size\n    if int(side_kpt[1, 0]) < (side_im_width / 2):\n        seg_crop = segImg.crop((0, 0, int(side_kpt[8, 0]), side_im_height))\n    else:\n        seg_crop = segImg.crop((int(side_kpt[8, 0]), 0, side_im_width, side_im_height))\n\n    side_p1, side_p2 = adjust(seg_crop)\n    side_height = y_distance(side_p1, side_p2)\n    side_Length_shoulderbone = round(np.linalg.norm(side_kpt[2] - side_kpt[1]))\n    side_F_Girth = round(np.linalg.norm(side_kpt[4] - side_kpt[3]))\n    side_R_Girth = round(np.linalg.norm(side_kpt[8] - side_kpt[7]))\n    rear_width = round(np.linalg.norm(rear_kpt[1] - rear_kpt[0]))\n    actual_width = rear_width * (side_height / rear_height)\n    #side_Length_shoulderbone,side_F_Girth,\tside_R_Girth, sticker, cattle , actual_width\n    return [side_Length_shoulderbone, side_F_Girth, side_R_Girth, sticker, cattle, actual_width]\n\ndef predict_weight(weight_model, features, sticker, cattle):\n    print('side_Length_shoulderbone,side_F_Girth,\tside_R_Girth, sticker, cattle , actual_width')\n    print(features)\n    predicted_cattle_weight = float(weight_model.predict([features]))\n    ratio = cattle / sticker\n\n    adjustment_factors = [\n        (50, 0.68), (55, 0.57), (60, 0.48), (65, 0.40), (67, 0.28),\n        (70, 0), (72, 0.25), (75, 0.35), (80, 0.45), (85, 0.55),\n        (90, 0.65), (95, 0.75), (100, 0.85), (105, 0.95), (110, 1.05),\n        (115, 1.15), (120, 1.25), (125, 1.35), (130, 1.45), (135, 1.55),\n        (140, 1.65), (145, 1.75), (150, 1.95), (155, 2.05), (160, 2.20),\n        (165, 2.35), (170, 2.45), (180, 2.55), (190, 2.65), (200, 2.75),\n        (210, 2.85), (220, 2.95), (230, 3.05)\n    ]\n\n    for threshold, factor in adjustment_factors:\n        if ratio < threshold:\n            predicted_cattle_weight += ratio * factor\n            break\n    else:\n        predicted_cattle_weight = 0\n        status = \"Sorry! This cattle can't be handled right now.\"\n        return {\"weight\": predicted_cattle_weight, \"ratio\": ratio, \"remarks\": status}\n\n    return {\"weight\": predicted_cattle_weight, \"ratio\": ratio, \"remarks\": \"ok\"}\n\ndef predict(side_fname, rear_fname):\n    print(torch.__version__, torch.cuda.is_available())\n\n    try:\n        seg_model, rear_pose_model, side_pose_model, weight_model = load_models()\n        rear_pose_results, rear_seg_result = process_image(rear_pose_model, seg_model, rear_fname)\n        side_pose_results, side_seg_result = process_image(side_pose_model, seg_model, side_fname)\n\n        # Debugging: print shapes and types of results\n#         print(f\"rear_pose_results: {rear_pose_results}\")\n#         print(f\"rear_seg_result: {rear_seg_result}\")\n#         print(f\"side_pose_results: {side_pose_results}\")\n#         print(f\"side_seg_result: {side_seg_result}\")\n#         print(side_seg_result[0])\n        for i in range(len(side_seg_result[0][1])):\n            for j in range(len(side_seg_result[0][1][i])):\n                if(side_seg_result[0][1][i][j]==1): \n                    side_seg_result[0][1][i][j]=2\n        seg = np.asarray(side_seg_result[0])\n        print(f\"seg shape: {seg.shape}, seg dtype: {seg.dtype}\")\n        \n        sticker = (seg == 2).sum()\n        cattle = (seg == 1).sum()\n\n        print(f\"sticker count: {sticker}, cattle count: {cattle}\")\n\n        if sticker < 100:\n            return {\"weight\": 0, \"ratio\": cattle / sticker, \"remarks\": \"Please apply sticker correctly.\"}\n        features = calculate_features(rear_pose_results, side_pose_results, side_seg_result, rear_seg_result, sticker, cattle)\n        return predict_weight(weight_model, features, sticker, cattle)\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {\"weight\": 0, \"ratio\": 0, \"remarks\": \"An error occurred during processing.\"}","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:19:31.806429Z","iopub.execute_input":"2024-07-06T09:19:31.807030Z","iopub.status.idle":"2024-07-06T09:19:31.851546Z","shell.execute_reply.started":"2024-07-06T09:19:31.806994Z","shell.execute_reply":"2024-07-06T09:19:31.850261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict('/kaggle/input/cow-side-back/1.0_s_117_10.0_F.jpg','/kaggle/input/cow-side-back/1.0_r_117_10.0_F.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:20:36.454709Z","iopub.execute_input":"2024-07-06T09:20:36.455176Z","iopub.status.idle":"2024-07-06T09:20:50.191077Z","shell.execute_reply.started":"2024-07-06T09:20:36.455140Z","shell.execute_reply":"2024-07-06T09:20:50.189961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_filename = '/kaggle/input/cattle-4-models/Final 4 model weights/model_v3.joblib'\nweight_model = joblib.load(weight_filename)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:28:18.748430Z","iopub.execute_input":"2024-07-06T09:28:18.748848Z","iopub.status.idle":"2024-07-06T09:28:19.132610Z","shell.execute_reply.started":"2024-07-06T09:28:18.748813Z","shell.execute_reply":"2024-07-06T09:28:19.130181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"float(weight_model.predict([[467, 477, 436, 2166, 106132, 355.49612403100775]]))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T09:28:36.569245Z","iopub.execute_input":"2024-07-06T09:28:36.570169Z","iopub.status.idle":"2024-07-06T09:28:36.635176Z","shell.execute_reply.started":"2024-07-06T09:28:36.570124Z","shell.execute_reply":"2024-07-06T09:28:36.633940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_pose_side_yolo = '/kaggle/input/cattle-4-models/Final 4 model weights/best_pose_side_yolov8l.pt'\nside_pose_model = YOLO(path_to_pose_side_yolo)\ndef inference_yolov8_pose(pose_model, img):\n    # Perform pose estimation using the pose model\n    results = pose_model.predict(img)\n    keypoints = []\n    for result in results:\n        keypoints.append({\n            \"keypoints\": np.array(result.keypoints.data.cpu(), dtype=np.float32)  # Ensure keypoints are on CPU and float32\n        })\n    return keypoints\n\nnp.asarray(inference_yolov8_pose(side_pose_model,'/kaggle/input/cow-side-back/1.0_s_117_10.0_F.jpg')[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:37:08.553921Z","iopub.execute_input":"2024-07-06T07:37:08.554344Z","iopub.status.idle":"2024-07-06T07:37:12.013614Z","shell.execute_reply.started":"2024-07-06T07:37:08.554313Z","shell.execute_reply":"2024-07-06T07:37:12.012202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab = [np.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]])]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:16:41.801272Z","iopub.execute_input":"2024-07-06T07:16:41.801689Z","iopub.status.idle":"2024-07-06T07:16:41.807697Z","shell.execute_reply.started":"2024-07-06T07:16:41.801656Z","shell.execute_reply":"2024-07-06T07:16:41.806502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seg = np.asarray(ab[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:16:42.121350Z","iopub.execute_input":"2024-07-06T07:16:42.121772Z","iopub.status.idle":"2024-07-06T07:16:42.127587Z","shell.execute_reply.started":"2024-07-06T07:16:42.121743Z","shell.execute_reply":"2024-07-06T07:16:42.126390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(seg==1).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:16:43.161513Z","iopub.execute_input":"2024-07-06T07:16:43.161953Z","iopub.status.idle":"2024-07-06T07:16:43.171531Z","shell.execute_reply.started":"2024-07-06T07:16:43.161918Z","shell.execute_reply":"2024-07-06T07:16:43.169976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab = ab.reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:42.833543Z","iopub.execute_input":"2024-07-06T07:10:42.833950Z","iopub.status.idle":"2024-07-06T07:10:42.839242Z","shell.execute_reply.started":"2024-07-06T07:10:42.833919Z","shell.execute_reply":"2024-07-06T07:10:42.837866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = set()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:49.459339Z","iopub.execute_input":"2024-07-06T07:10:49.459824Z","iopub.status.idle":"2024-07-06T07:10:49.465393Z","shell.execute_reply.started":"2024-07-06T07:10:49.459788Z","shell.execute_reply":"2024-07-06T07:10:49.464157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ab:\n    st.add(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:10:59.526592Z","iopub.execute_input":"2024-07-06T07:10:59.527020Z","iopub.status.idle":"2024-07-06T07:10:59.756308Z","shell.execute_reply.started":"2024-07-06T07:10:59.526987Z","shell.execute_reply":"2024-07-06T07:10:59.754937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:11:02.287892Z","iopub.execute_input":"2024-07-06T07:11:02.289151Z","iopub.status.idle":"2024-07-06T07:11:02.296084Z","shell.execute_reply.started":"2024-07-06T07:11:02.289107Z","shell.execute_reply":"2024-07-06T07:11:02.294750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_pose_rear_yolo = '/kaggle/input/cattle-4-models/Final 4 model weights/best_seg_yolov8l.pt'\nseg_model = YOLO(path_to_pose_rear_yolo)\ndef inference_yolov8_seg(seg_model, img):\n    # Perform segmentation using the segmentation model\n    results = seg_model.predict(img)\n    segmentation_masks = []\n    for result in results:\n        segmentation_masks.append(np.array(result.masks.data.cpu(), dtype=np.uint8))  # Ensure masks are on CPU and uint8\n    return segmentation_masks\ninference_yolov8_seg(seg_model,'/kaggle/input/cow-side-back/1.0_s_117_10.0_F.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T07:58:42.694140Z","iopub.execute_input":"2024-07-06T07:58:42.695084Z","iopub.status.idle":"2024-07-06T07:58:46.437281Z","shell.execute_reply.started":"2024-07-06T07:58:42.695045Z","shell.execute_reply":"2024-07-06T07:58:46.436188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab = seg_model.predict('/kaggle/input/cow-side-back/1.0_s_117_10.0_F.jpg')[0].masks.data.numpy()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:32:50.819174Z","iopub.execute_input":"2024-07-06T08:32:50.819602Z","iopub.status.idle":"2024-07-06T08:32:52.311062Z","shell.execute_reply.started":"2024-07-06T08:32:50.819570Z","shell.execute_reply":"2024-07-06T08:32:52.309877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ab[0][0]","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:39:35.523012Z","iopub.execute_input":"2024-07-06T08:39:35.523454Z","iopub.status.idle":"2024-07-06T08:39:35.533734Z","shell.execute_reply.started":"2024-07-06T08:39:35.523421Z","shell.execute_reply":"2024-07-06T08:39:35.532581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(ab[0][1].reshape(-1)==1).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:40:23.048669Z","iopub.execute_input":"2024-07-06T08:40:23.049125Z","iopub.status.idle":"2024-07-06T08:40:23.057793Z","shell.execute_reply.started":"2024-07-06T08:40:23.049088Z","shell.execute_reply":"2024-07-06T08:40:23.056318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(ab[1])):\n    for j in range(len(ab[1][i])):\n        if(ab[1][i][j]==1): \n            ab[1][i][j]=2","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:40:24.658648Z","iopub.execute_input":"2024-07-06T08:40:24.659091Z","iopub.status.idle":"2024-07-06T08:40:25.842738Z","shell.execute_reply.started":"2024-07-06T08:40:24.659059Z","shell.execute_reply":"2024-07-06T08:40:25.841332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(ab[1].reshape(-1)==2).sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:40:27.143948Z","iopub.execute_input":"2024-07-06T08:40:27.144405Z","iopub.status.idle":"2024-07-06T08:40:27.152792Z","shell.execute_reply.started":"2024-07-06T08:40:27.144369Z","shell.execute_reply":"2024-07-06T08:40:27.151485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = set()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:04:05.976649Z","iopub.execute_input":"2024-07-06T08:04:05.977144Z","iopub.status.idle":"2024-07-06T08:04:05.982479Z","shell.execute_reply.started":"2024-07-06T08:04:05.977110Z","shell.execute_reply":"2024-07-06T08:04:05.981303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ab:\n    st.add(i)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:04:16.615646Z","iopub.execute_input":"2024-07-06T08:04:16.616056Z","iopub.status.idle":"2024-07-06T08:04:16.856977Z","shell.execute_reply.started":"2024-07-06T08:04:16.616011Z","shell.execute_reply":"2024-07-06T08:04:16.855657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:04:19.421298Z","iopub.execute_input":"2024-07-06T08:04:19.421737Z","iopub.status.idle":"2024-07-06T08:04:19.430651Z","shell.execute_reply.started":"2024-07-06T08:04:19.421688Z","shell.execute_reply":"2024-07-06T08:04:19.429434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.array([1,2,3,4])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T08:31:08.376761Z","iopub.execute_input":"2024-07-06T08:31:08.377206Z","iopub.status.idle":"2024-07-06T08:31:08.382911Z","shell.execute_reply.started":"2024-07-06T08:31:08.377171Z","shell.execute_reply":"2024-07-06T08:31:08.381526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}