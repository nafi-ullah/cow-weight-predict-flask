{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":6104759,"sourceType":"datasetVersion","datasetId":3496963},{"sourceId":6322617,"sourceType":"datasetVersion","datasetId":3638620},{"sourceId":6582986,"sourceType":"datasetVersion","datasetId":3800496}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openmim\n!pip install -q /kaggle/input/frozen-packages-mmdetection/mmcv-2.0.1-cp310-cp310-linux_x86_64.whl\n\n!git clone https://github.com/open-mmlab/mmdetection.git\n%cd mmdetection\n!pip install -e .\n\n%cd ..\n!git clone https://github.com/open-mmlab/mmpose.git\n%cd mmpose\n!pip install -e .\n\n!mkdir checkpoint\n!mkdir outputs\n!mkdir data\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-03T13:06:05.923869Z","iopub.execute_input":"2024-07-03T13:06:05.924192Z","iopub.status.idle":"2024-07-03T13:07:49.171518Z","shell.execute_reply.started":"2024-07-03T13:06:05.924164Z","shell.execute_reply":"2024-07-03T13:07:49.170360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython import display\nimport mmcv\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint('MMCV版本', mmcv.__version__)\n%cd /kaggle/working/mmdetection\nimport mmdet\nprint('mmdetection版本', mmdet.__version__)\n%cd /kaggle/working/mmpose\nimport mmpose\nprint('mmpose版本', mmpose.__version__)\nprint('CUDA版本', get_compiling_cuda_version())\nprint('编译器版本', get_compiler_version())","metadata":{"execution":{"iopub.status.busy":"2024-07-03T13:08:13.903112Z","iopub.execute_input":"2024-07-03T13:08:13.903518Z","iopub.status.idle":"2024-07-03T13:08:25.375650Z","shell.execute_reply.started":"2024-07-03T13:08:13.903486Z","shell.execute_reply":"2024-07-03T13:08:25.374708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nimport joblib\nfrom PIL import Image\nfrom mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n                        process_mmdet_results)\nfrom mmdet.apis import inference_detector, init_detector\nfrom mmseg.apis import init_segmentor, inference_segmentor\nimport os\nimport math\nimport torch, torchvision","metadata":{"execution":{"iopub.status.busy":"2024-07-03T13:09:33.756496Z","iopub.execute_input":"2024-07-03T13:09:33.757137Z","iopub.status.idle":"2024-07-03T13:09:34.753142Z","shell.execute_reply.started":"2024-07-03T13:09:33.757108Z","shell.execute_reply":"2024-07-03T13:09:34.751944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_PATH = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nJSON_PATH =  '/kaggle/input/hrnettest3333/rebatrain_keypoints.json'\n\n\nVAL_IMAGE_PATH = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nVAL_JSON_PATH =  '/kaggle/input/hrnettest3333/rebatrain_keypoints.json'","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:29:29.298855Z","iopub.execute_input":"2023-10-01T08:29:29.299657Z","iopub.status.idle":"2023-10-01T08:29:29.304079Z","shell.execute_reply.started":"2023-10-01T08:29:29.299621Z","shell.execute_reply":"2023-10-01T08:29:29.303277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/mmpose/data","metadata":{"execution":{"iopub.status.busy":"2023-10-01T06:48:26.088957Z","iopub.execute_input":"2023-10-01T06:48:26.089345Z","iopub.status.idle":"2023-10-01T06:48:27.080171Z","shell.execute_reply.started":"2023-10-01T06:48:26.089286Z","shell.execute_reply":"2023-10-01T06:48:27.078866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/mmpose/data/*","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:29:59.490989Z","iopub.execute_input":"2023-10-01T08:29:59.491338Z","iopub.status.idle":"2023-10-01T08:30:00.631579Z","shell.execute_reply.started":"2023-10-01T08:29:59.49131Z","shell.execute_reply":"2023-10-01T08:30:00.630269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport io\nimport json\nimport shutil\nimport random\nimport numpy as np\nfrom pathlib import Path\n\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom mmengine import Config\n\nfrom pycocotools.coco import COCO\n\nimport os\nwith open(JSON_PATH) as jfff:\n    ccc = json.load(jfff)\ni_names = [dx['image_file_name'] for dx in ccc]\ndef separate_files(og_folder, trans_folder):\n    \n    image_folder = os.path.join(trans_folder, 'images')\n    ann_folder = os.path.join(trans_folder, 'ann')\n    os.makedirs(image_folder, exist_ok=True)\n    os.makedirs(ann_folder, exist_ok=True)\n    for file in tqdm(os.listdir(data_folder)):\n        if file not in i_names:\n            continue\n        if file.endswith('.jpg'):\n            source_path = os.path.join(og_folder, file)\n            target_path = os.path.join(image_folder, file)\n            shutil.copy(source_path, target_path)\n       \n    \n    \ndata_folder = IMAGE_PATH\ntrans_folder = './data'\n\nseparate_files(data_folder, trans_folder)\n\nimport json\ndef read_file_as_list(file_path):\n    with open(file_path, 'r') as file:\n        content = file.read()\n        key_point = [int(num) for num in content.split()]\n        key_num = key_point[0]\n        key_point.pop(0)\n        for i in range(2, len(key_point) + len(key_point)//2, 2 + 1):\n            key_point.insert(i, 2)\n    return key_num,key_point\n\ndef get_image_size(image_path):\n    with Image.open(image_path) as img:\n        width, height = img.size\n    return width, height\n\n\ndef read_file_as_list(file_path):\n    key_point = file_path['keypoints'] \n    key_num = 18\n    return key_num,key_point\n\n\ndef coco_structure(train_file,train_image_dir,val_file,val_image_dir):\n    coco = dict()\n    coco['images'] = []\n    coco['annotations'] = []\n    coco['categories'] = []\n    coco['categories'].append(dict(supercategory = 'person',id = 1,name = 'person',\n                               keypoints = ['forehead', 'nose',  'neck', 'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'center_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle','left_hand','right_hand'],\n                               skeleton = [(0, 1),  # forehead to nose\n                                            (1, 2),  # nose to neck\n                                            (2, 3),  # neck to left shoulder\n                                            (2, 4),  # neck to right shoulder\n                                            (3, 5),  # left shoulder to left elbow\n                                            (4, 6),  # right shoulder to right elbow\n                                            (5, 7),  # left elbow to left wrist\n                                            (6, 8),  # right elbow to right wrist\n                                            (2, 9),  # neck to center hip\n                                            (9, 10),  # center hip to left hip\n                                            (9, 11),  # center hip to right hip\n                                            (10, 12),  # left hip to left knee\n                                            (11, 13),  # right hip to right knee\n                                            (12, 14),  # left knee to left ankle\n                                            (13, 15),  # right knee to right ankle\n                                            (7, 16),  # left wrist to left hand\n                                            (8, 17)  # right wrist to right hand\n                                          ]))\n    with open(train_file,'r') as file:\n        ann_list = json.load(file)\n            \n    id = 0\n    for ann_file in tqdm(ann_list):\n        key_num,key_point = read_file_as_list(ann_file)\n        img_file_name = ann_file['image_file_name']\n        img_id = ann_file['image_id']\n        if key_num == 18:\n            image_name = img_file_name\n            image_id = img_id\n            height, width = get_image_size(os.path.join(train_image_dir, image_name))\n            image = {\"id\": id, \"file_name\": image_name, \"height\": height, \"width\": width}\n            coco['images'].append(image)\n            key_dict = dict(category_id = 1, segmentation = [], iscrowd = 0, image_id = id, \n                    id = id, bbox = [0, 0, width, height], area = width * height, num_keypoints = key_num, keypoints = key_point)\n            coco['annotations'].append(key_dict)\n            id = id + 1\n    with open(val_file,'r') as file:\n        ann_list = json.load(file)\n            \n    id = 0\n    for ann_file in tqdm(ann_list):\n        key_num,key_point = read_file_as_list(ann_file)\n        img_file_name = ann_file['image_file_name']\n        img_id = ann_file['image_id']\n        if key_num == 18:\n            image_name = img_file_name\n            image_id = img_id\n            height, width = get_image_size(os.path.join(val_image_dir, image_name))\n            image = {\"id\": id, \"file_name\": image_name, \"height\": height, \"width\": width}\n            coco['images'].append(image)\n            key_dict = dict(category_id = 1, segmentation = [], iscrowd = 0, image_id = id, \n                    id = id, bbox = [0, 0, width, height], area = width * height, num_keypoints = key_num, keypoints = key_point)\n            coco['annotations'].append(key_dict)\n            id = id + 1\n    print(\"Total annoatations: \",len(coco['annotations']))\n    return coco\n\ntrain_file = coco_structure(JSON_PATH,IMAGE_PATH,VAL_JSON_PATH,VAL_IMAGE_PATH)\n\noutput_file_path =  './data/annotations.json'\nwith open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n    json.dump(train_file, output_file, ensure_ascii=True, indent=4)\n\n    \n\ndef split_coco_dataset(coco_json_path: str, save_dir: str, ratios: list,\n                       shuffle: bool, seed: int):\n    if not Path(coco_json_path).exists():\n        raise FileNotFoundError(f'Can not not found {coco_json_path}')\n\n    if not Path(save_dir).exists():\n        Path(save_dir).mkdir(parents=True)\n\n    ratios = np.array(ratios) / np.array(ratios).sum()\n\n    if len(ratios) == 2:\n        ratio_train, ratio_test = ratios\n        print(\"Ratio train:\",ratio_train)\n        ratio_val = 0\n        train_type = 'trainval'\n    elif len(ratios) == 3:\n        ratio_train, ratio_val, ratio_test = ratios\n        train_type = 'train'\n    else:\n        raise ValueError('ratios must set 2 or 3 group!')\n\n    coco = COCO(coco_json_path)\n    coco_image_ids = coco.getImgIds()\n\n    val_image_num = int(len(coco_image_ids) * ratio_val)\n    test_image_num = int(len(coco_image_ids) * ratio_test)\n    train_image_num = len(coco_image_ids) - val_image_num - test_image_num\n    print('Split info: ====== \\n'\n          f'Train ratio = {ratio_train}, number = {train_image_num}\\n'\n          f'Val ratio = {ratio_val}, number = {val_image_num}\\n'\n          f'Test ratio = {ratio_test}, number = {test_image_num}')\n\n    seed = int(seed)\n    if seed != -1:\n        print(f'Set the global seed: {seed}')\n        np.random.seed(seed)\n\n    if shuffle:\n        print('shuffle dataset.')\n        random.shuffle(coco_image_ids)\n\n    train_image_ids = coco_image_ids[:train_image_num]\n    if val_image_num != 0:\n        val_image_ids = coco_image_ids[train_image_num:train_image_num +\n                                       val_image_num]\n    else:\n        val_image_ids = None\n    test_image_ids = coco_image_ids[train_image_num + val_image_num:]\n\n    categories = coco.loadCats(coco.getCatIds())\n    for img_id_list in [train_image_ids, val_image_ids, test_image_ids]:\n        if img_id_list is None:\n            continue\n\n        img_dict = {\n            'images': coco.loadImgs(ids=img_id_list),\n            'categories': categories,\n            'annotations': coco.loadAnns(coco.getAnnIds(imgIds=img_id_list))\n        }\n\n        if img_id_list == train_image_ids:\n            json_file_path = Path(save_dir, f'{train_type}.json')\n        elif img_id_list == val_image_ids:\n            json_file_path = Path(save_dir, 'val.json')\n        elif img_id_list == test_image_ids:\n            json_file_path = Path(save_dir, 'test.json')\n        else:\n            raise ValueError('img_id_list ERROR!')\n\n        print(f'Saving json to {json_file_path}')\n        with open(json_file_path, 'w') as f_json:\n            json.dump(img_dict, f_json, ensure_ascii=False, indent=2)\n\n    print('All done!')\nsplit_coco_dataset('./data/annotations.json', './data', [0.95,0.5], True, 2023)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:30:16.429865Z","iopub.execute_input":"2023-10-01T08:30:16.430315Z","iopub.status.idle":"2023-10-01T08:30:19.973401Z","shell.execute_reply.started":"2023-10-01T08:30:16.430256Z","shell.execute_reply":"2023-10-01T08:30:19.972376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:07:37.812642Z","iopub.execute_input":"2023-09-29T15:07:37.813302Z","iopub.status.idle":"2023-09-29T15:07:38.174421Z","shell.execute_reply.started":"2023-09-29T15:07:37.813272Z","shell.execute_reply":"2023-09-29T15:07:38.173473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:07:39.417259Z","iopub.execute_input":"2023-09-29T15:07:39.417634Z","iopub.status.idle":"2023-09-29T15:07:39.429339Z","shell.execute_reply.started":"2023-09-29T15:07:39.417607Z","shell.execute_reply":"2023-09-29T15:07:39.428082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/mmpose/work_dir/*","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:19:10.90139Z","iopub.execute_input":"2023-10-01T09:19:10.901841Z","iopub.status.idle":"2023-10-01T09:19:12.220132Z","shell.execute_reply.started":"2023-10-01T09:19:10.901802Z","shell.execute_reply":"2023-10-01T09:19:12.218807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_config = \"\"\"\n_base_ = ['mmpose::_base_/default_runtime.py']\n\ndataset_type = 'CocoDataset'\ndata_mode = 'topdown'\ndata_root = './data/'\nwork_dir = './work_dir'\nbackend_args = dict(backend='local') # data storage backend\n\n#resume = True\ndataset_info = {\n    'dataset_name':'Keypoint_reba',\n    'classes':'person',\n    'paper_info':{\n        'author':'Reba pose',\n        'title':'Reba Keypoints Detection',\n    },\n    'keypoint_info':{\n    0: {'name': 'Forehead', 'id': 0, 'color': [255, 0, 0], 'type': '', 'swap': ''},\n    1: {'name': 'Nose', 'id': 1, 'color': [255, 127, 0], 'type': '', 'swap': ''},\n    2: {'name': 'Neck', 'id': 2, 'color': [255, 255, 0], 'type': '', 'swap': ''},\n    3: {'name': 'Left Shoulder', 'id': 3, 'color': [0, 255, 0], 'type': '', 'swap': ''},\n    4: {'name': 'Right Shoulder', 'id': 4, 'color': [0, 255, 255], 'type': '', 'swap': ''},\n    5: {'name': 'Left Elbow', 'id': 5, 'color': [0, 0, 255], 'type': '', 'swap': ''},\n    6: {'name': 'Right Elbow', 'id': 6, 'color': [139, 0, 255], 'type': '', 'swap': ''},\n    7: {'name': 'Left Wrist', 'id': 7, 'color': [255, 0, 255], 'type': '', 'swap': ''},\n    8: {'name': 'Right Wrist', 'id': 8, 'color': [160, 82, 45], 'type': '', 'swap': ''},\n    9: {'name': 'Left Hip', 'id': 9, 'color': [255, 0, 0], 'type': '', 'swap': ''},\n    10: {'name': 'Center Hip', 'id': 10, 'color': [255, 127, 0], 'type': '', 'swap': ''},\n    11: {'name': 'Right Hip', 'id': 11, 'color': [255, 255, 0], 'type': '', 'swap': ''},\n    12: {'name': 'Left Knee', 'id': 12, 'color': [0, 255, 0], 'type': '', 'swap': ''},\n    13: {'name': 'Right Knee', 'id': 13, 'color': [0, 255, 255], 'type': '', 'swap': ''},\n    14: {'name': 'Left Ankle', 'id': 14, 'color': [0, 0, 255], 'type': '', 'swap': ''},\n    15: {'name': 'Right Ankle', 'id': 15, 'color': [139, 0, 255], 'type': '', 'swap': ''},\n    16: {'name': 'Left Hand', 'id': 16, 'color': [255, 0, 255], 'type': '', 'swap': ''},\n    17: {'name': 'Right Hand', 'id': 17, 'color': [160, 82, 45], 'type': '', 'swap': ''}\n},\n    'skeleton_info': {\n    0: {'link': ('Forehead', 'Nose'), 'id': 0, 'color': [255, 0, 0]},\n    1: {'link': ('Nose', 'Neck'), 'id': 1, 'color': [255, 0, 0]},\n    2: {'link': ('Neck', 'Left Shoulder'), 'id': 2, 'color': [255, 0, 0]},\n    3: {'link': ('Neck', 'Right Shoulder'), 'id': 3, 'color': [255, 0, 0]},\n    4: {'link': ('Left Shoulder', 'Left Elbow'), 'id': 4, 'color': [255, 0, 0]},\n    5: {'link': ('Right Shoulder', 'Right Elbow'), 'id': 5, 'color': [255, 0, 0]},\n    6: {'link': ('Left Elbow', 'Left Wrist'), 'id': 6, 'color': [255, 0, 0]},\n    7: {'link': ('Right Elbow', 'Right Wrist'), 'id': 7, 'color': [255, 0, 0]},\n    8: {'link': ('Neck', 'Left Hip'), 'id': 8, 'color': [255, 0, 0]},\n    9: {'link': ('Left Hip', 'Center Hip'), 'id': 9, 'color': [255, 0, 0]},\n    10: {'link': ('Center Hip', 'Right Hip'), 'id': 10, 'color': [255, 0, 0]},\n    11: {'link': ('Left Hip', 'Left Knee'), 'id': 11, 'color': [255, 0, 0]},\n    12: {'link': ('Right Hip', 'Right Knee'), 'id': 12, 'color': [255, 0, 0]},\n    13: {'link': ('Left Knee', 'Left Ankle'), 'id': 13, 'color': [255, 0, 0]},\n    14: {'link': ('Right Knee', 'Right Ankle'), 'id': 14, 'color': [255, 0, 0]},\n    15: {'link': ('Left Wrist', 'Left Hand'), 'id': 15, 'color': [255, 0, 0]},\n    16: {'link': ('Right Wrist', 'Right Hand'), 'id': 16, 'color': [255, 0, 0]},\n}\n}\n\nNUM_KEYPOINTS = len(dataset_info['keypoint_info'])\ndataset_info['joint_weights'] = [1.0] * NUM_KEYPOINTS\ndataset_info['sigmas'] = [0.025] * NUM_KEYPOINTS\n\nmax_epochs = 300\nval_interval = 2\ntrain_cfg = {'by_epoch':True,'max_epochs': max_epochs, 'val_begin':2, 'val_interval': val_interval}\ntrain_batch_size = 64\nval_batch_size = 32\nstage2_num_epochs = 10\nbase_lr = 0.001\nrandomness = dict(seed=2023)\n\nparam_scheduler = [\n    dict( # warmup strategy\n        type='LinearLR', begin=0, end=20, start_factor=0.001, by_epoch=False),\n    dict( # scheduler\n        type='MultiStepLR',\n        begin=20,\n        end=210,\n        milestones=[170, 200],\n        gamma=0.1,\n        by_epoch=True)\n]\noptim_wrapper = dict(optimizer=dict(type='Adam', lr=0.001)) # optimizer and initial lr\n\n\n#auto_scale_lr = dict(base_batch_size=32) # auto scale the lr according to batch size\n# automatically scaling LR based on the actual training batch size\n# auto_scale_lr = dict(base_batch_size=1024)\n\n# codec settings\n# input_size\n# input_size\ncodec = dict(\n    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)\n\nmodel = dict(\n    type='TopdownPoseEstimator', # Macro model structure\n    data_preprocessor=dict( # data normalization and channel transposition\n        type='PoseDataPreprocessor',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True),\n    backbone=dict( # config of backbone\n        type='HRNet',\n        in_channels=3,\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256))),\n        init_cfg=dict(\n            type='Pretrained', # load pretrained weights to backbone\n            checkpoint='https://download.openmmlab.com/mmpose'\n            '/pretrain_models/hrnet_w32-36af842e.pth'),\n    ),\n    head=dict( # config of head\n        type='HeatmapHead',\n        in_channels=32,\n        out_channels=NUM_KEYPOINTS,\n        deconv_out_channels=None,\n        loss=dict(type='KeypointMSELoss', use_target_weight=True), # config of loss function\n        decoder=codec), # get decoder from codec\n    test_cfg=dict(\n        flip_test=True, # flag of flip test\n        flip_mode='heatmap', # heatmap flipping\n        shift_heatmap=True,  # shift the flipped heatmap several pixels to get a better performance\n    ))\n\n# pipelines\ntrain_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    # dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform', scale_factor=[0.8, 1.2], rotate_factor=30),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='ChannelShuffle', p=0.5),\n            dict(type='CLAHE', p=0.5),\n            # dict(type='Downscale', scale_min=0.7, scale_max=0.9, p=0.2),\n            dict(type='ColorJitter', p=0.5),\n            dict(\n                type='CoarseDropout',\n                max_holes=4,\n                max_height=0.3,\n                max_width=0.3,\n                min_holes=1,\n                min_height=0.2,\n                min_width=0.2,\n                p=0.5),\n        ]),\n    dict(type='GenerateTarget', encoder=codec),\n    dict(type='PackPoseInputs')\n]\n\nval_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='PackPoseInputs')\n]\n\ntrain_pipeline_stage2 = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform',\n        shift_factor=0.,\n        scale_factor=[0.75, 1.25],\n        rotate_factor=60),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='Blur', p=0.1),\n            dict(type='MedianBlur', p=0.1),\n            dict(\n                type='CoarseDropout',\n                max_holes=1,\n                max_height=0.4,\n                max_width=0.4,\n                min_holes=1,\n                min_height=0.2,\n                min_width=0.2,\n                p=0.5),\n        ]),\n    dict(type='GenerateTarget', encoder=codec),\n    dict(type='PackPoseInputs')\n]\n\n# data loaders\ntrain_dataloader = dict(\n    batch_size=train_batch_size,\n    num_workers=2,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=dataset_info,\n        data_mode=data_mode,\n        ann_file='trainval.json',\n        data_prefix=dict(img='images/'),\n        pipeline=train_pipeline,\n    ))\nval_dataloader = dict(\n    batch_size=val_batch_size,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=dataset_info,\n        data_mode=data_mode,\n        ann_file='test.json',\n        data_prefix=dict(img='images/'),\n        pipeline=val_pipeline,\n    ))\ntest_dataloader = val_dataloader\n\ndefault_hooks = {\n    'checkpoint': {'save_best': 'PCK','rule': 'greater','max_keep_ckpts': 2},\n    'logger': {'interval': 50}\n}\n\ncustom_hooks = [\n    dict(\n        type='EMAHook',\n        ema_type='ExpMomentumEMA',\n        momentum=0.0002,\n        update_buffers=True,\n        priority=49),\n    dict(\n        type='mmdet.PipelineSwitchHook',\n        switch_epoch=max_epochs - stage2_num_epochs,\n        switch_pipeline=train_pipeline_stage2)\n]\n\n# evaluators\nval_evaluator = [\n    dict(type='CocoMetric', ann_file=data_root + 'test.json'),\n    dict(type='PCKAccuracy'),\n    dict(type='AUC'),\n    dict(type='NME', norm_mode='keypoint_distance', keypoint_indices=[0, 1])\n]\n\ntest_evaluator = val_evaluator\n\"\"\"\nconfig = './configs/reba_keypoint.py'\nwith io.open(config, 'w', encoding='utf-8') as f:\n    f.write(custom_config)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T09:22:37.125531Z","iopub.execute_input":"2023-10-01T09:22:37.125953Z","iopub.status.idle":"2023-10-01T09:22:37.138078Z","shell.execute_reply.started":"2023-10-01T09:22:37.12592Z","shell.execute_reply":"2023-10-01T09:22:37.136826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-10-01T08:54:53.436699Z","iopub.execute_input":"2023-10-01T08:54:53.437037Z","iopub.status.idle":"2023-10-01T08:54:54.41859Z","shell.execute_reply.started":"2023-10-01T08:54:53.437009Z","shell.execute_reply":"2023-10-01T08:54:54.417272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python tools/train.py {config}","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-01T09:19:30.117928Z","iopub.execute_input":"2023-10-01T09:19:30.118299Z","iopub.status.idle":"2023-10-01T09:19:57.287026Z","shell.execute_reply.started":"2023-10-01T09:19:30.118271Z","shell.execute_reply":"2023-10-01T09:19:57.285825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_config = \"\"\"\n_base_ = ['mmpose::_base_/default_runtime.py']\n\ndataset_type = 'CocoDataset'\ndata_mode = 'topdown'\ndata_root = './data/'\nwork_dir = './work_dir'\nbackend_args = dict(backend='local') # data storage backend\n\n\ndataset_info = {\n    'dataset_name':'Keypoint_reba',\n    'classes':'person',\n    'paper_info':{\n        'author':'Reba pos',\n        'title':'Reba Keypoints Detection',\n    },\n    'keypoint_info':{\n    0: {'name': 'Forehead', 'id': 0, 'color': [255, 0, 0], 'type': '', 'swap': ''},\n    1: {'name': 'Nose', 'id': 1, 'color': [255, 127, 0], 'type': '', 'swap': ''},\n    2: {'name': 'Neck', 'id': 2, 'color': [255, 255, 0], 'type': '', 'swap': ''},\n    3: {'name': 'Left Shoulder', 'id': 3, 'color': [0, 255, 0], 'type': '', 'swap': ''},\n    4: {'name': 'Right Shoulder', 'id': 4, 'color': [0, 255, 255], 'type': '', 'swap': ''},\n    5: {'name': 'Left Elbow', 'id': 5, 'color': [0, 0, 255], 'type': '', 'swap': ''},\n    6: {'name': 'Right Elbow', 'id': 6, 'color': [139, 0, 255], 'type': '', 'swap': ''},\n    7: {'name': 'Left Wrist', 'id': 7, 'color': [255, 0, 255], 'type': '', 'swap': ''},\n    8: {'name': 'Right Wrist', 'id': 8, 'color': [160, 82, 45], 'type': '', 'swap': ''},\n    9: {'name': 'Left Hip', 'id': 9, 'color': [255, 0, 0], 'type': '', 'swap': ''},\n    10: {'name': 'Center Hip', 'id': 10, 'color': [255, 127, 0], 'type': '', 'swap': ''},\n    11: {'name': 'Right Hip', 'id': 11, 'color': [255, 255, 0], 'type': '', 'swap': ''},\n    12: {'name': 'Left Knee', 'id': 12, 'color': [0, 255, 0], 'type': '', 'swap': ''},\n    13: {'name': 'Right Knee', 'id': 13, 'color': [0, 255, 255], 'type': '', 'swap': ''},\n    14: {'name': 'Left Ankle', 'id': 14, 'color': [0, 0, 255], 'type': '', 'swap': ''},\n    15: {'name': 'Right Ankle', 'id': 15, 'color': [139, 0, 255], 'type': '', 'swap': ''},\n    16: {'name': 'Left Hand', 'id': 16, 'color': [255, 0, 255], 'type': '', 'swap': ''},\n    17: {'name': 'Right Hand', 'id': 17, 'color': [160, 82, 45], 'type': '', 'swap': ''}\n},\n    'skeleton_info': {\n    0: {'link': ('Forehead', 'Nose'), 'id': 0, 'color': [255, 0, 0]},\n    1: {'link': ('Nose', 'Neck'), 'id': 1, 'color': [255, 0, 0]},\n    2: {'link': ('Neck', 'Left Shoulder'), 'id': 2, 'color': [255, 0, 0]},\n    3: {'link': ('Neck', 'Right Shoulder'), 'id': 3, 'color': [255, 0, 0]},\n    4: {'link': ('Left Shoulder', 'Left Elbow'), 'id': 4, 'color': [255, 0, 0]},\n    5: {'link': ('Right Shoulder', 'Right Elbow'), 'id': 5, 'color': [255, 0, 0]},\n    6: {'link': ('Left Elbow', 'Left Wrist'), 'id': 6, 'color': [255, 0, 0]},\n    7: {'link': ('Right Elbow', 'Right Wrist'), 'id': 7, 'color': [255, 0, 0]},\n    8: {'link': ('Neck', 'Left Hip'), 'id': 8, 'color': [255, 0, 0]},\n    9: {'link': ('Left Hip', 'Center Hip'), 'id': 9, 'color': [255, 0, 0]},\n    10: {'link': ('Center Hip', 'Right Hip'), 'id': 10, 'color': [255, 0, 0]},\n    11: {'link': ('Left Hip', 'Left Knee'), 'id': 11, 'color': [255, 0, 0]},\n    12: {'link': ('Right Hip', 'Right Knee'), 'id': 12, 'color': [255, 0, 0]},\n    13: {'link': ('Left Knee', 'Left Ankle'), 'id': 13, 'color': [255, 0, 0]},\n    14: {'link': ('Right Knee', 'Right Ankle'), 'id': 14, 'color': [255, 0, 0]},\n    15: {'link': ('Left Wrist', 'Left Hand'), 'id': 15, 'color': [255, 0, 0]},\n    16: {'link': ('Right Wrist', 'Right Hand'), 'id': 16, 'color': [255, 0, 0]},\n}\n}\n\nNUM_KEYPOINTS = len(dataset_info['keypoint_info'])\ndataset_info['joint_weights'] = [1.0] * NUM_KEYPOINTS\ndataset_info['sigmas'] = [0.025] * NUM_KEYPOINTS\n\nmax_epochs = 300\nval_interval = 2\ntrain_cfg = {'by_epoch'=True,'max_epochs': max_epochs, 'val_begin':2, 'val_interval': val_interval}\ntrain_batch_size = 32\nval_batch_size = 32\nstage2_num_epochs = 10\nbase_lr = 0.001\nrandomness = dict(seed=2023)\n\noptim_wrapper = dict(\n    type='OptimWrapper',\n    optimizer=dict(type='Adam', lr=base_lr, weight_decay=0.0001),\n    paramwise_cfg=dict(\n        norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))\n\nparam_scheduler = [\n    dict(type='LinearLR', start_factor=1.0e-3, by_epoch=False, begin=0, end=600),\n    dict(\n        type='CosineAnnealingLR',\n        eta_min=base_lr * 0.05,\n        begin=max_epochs // 2,\n        end=max_epochs,\n        T_max=max_epochs // 2,\n        by_epoch=True,\n        convert_to_iter_based=True),\n]\n\n# automatically scaling LR based on the actual training batch size\n# auto_scale_lr = dict(base_batch_size=1024)\n\n# codec settings\n# input_size\n# input_size\ncodec = dict(\n    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)\n\nmodel = dict(\n    type='TopdownPoseEstimator', # Macro model structure\n    data_preprocessor=dict( # data normalization and channel transposition\n        type='PoseDataPreprocessor',\n        mean=[123.675, 116.28, 103.53],\n        std=[58.395, 57.12, 57.375],\n        bgr_to_rgb=True),\n    backbone=dict( # config of backbone\n        type='HRNet',\n        in_channels=3,\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256))),\n        init_cfg=dict(\n            type='Pretrained', # load pretrained weights to backbone\n            checkpoint='https://download.openmmlab.com/mmpose'\n            '/pretrain_models/hrnet_w32-36af842e.pth'),\n    ),\n    head=dict( # config of head\n        type='HeatmapHead',\n        in_channels=32,\n        out_channels=NUM_KEYPOINTS,\n        deconv_out_channels=None,\n        loss=dict(type='KeypointMSELoss', use_target_weight=True), # config of loss function\n        decoder=codec), # get decoder from codec\n    test_cfg=dict(\n        flip_test=True, # flag of flip test\n        flip_mode='heatmap', # heatmap flipping\n        shift_heatmap=True,  # shift the flipped heatmap several pixels to get a better performance\n    ))\n\n# pipelines\ntrain_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    # dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform', scale_factor=[0.8, 1.2], rotate_factor=30),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='ChannelShuffle', p=0.5),\n            dict(type='CLAHE', p=0.5),\n            # dict(type='Downscale', scale_min=0.7, scale_max=0.9, p=0.2),\n            dict(type='ColorJitter', p=0.5),\n            dict(\n                type='CoarseDropout',\n                max_holes=4,\n                max_height=0.3,\n                max_width=0.3,\n                min_holes=1,\n                min_height=0.2,\n                min_width=0.2,\n                p=0.5),\n        ]),\n    dict(type='GenerateTarget', encoder=codec),\n    dict(type='PackPoseInputs')\n]\n\nval_pipeline = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='PackPoseInputs')\n]\n\ntrain_pipeline_stage2 = [\n    dict(type='LoadImage', backend_args=backend_args),\n    dict(type='GetBBoxCenterScale'),\n    dict(type='RandomFlip', direction='horizontal'),\n    dict(type='RandomHalfBody'),\n    dict(\n        type='RandomBBoxTransform',\n        shift_factor=0.,\n        scale_factor=[0.75, 1.25],\n        rotate_factor=60),\n    dict(type='TopdownAffine', input_size=codec['input_size']),\n    dict(type='mmdet.YOLOXHSVRandomAug'),\n    dict(\n        type='Albumentation',\n        transforms=[\n            dict(type='Blur', p=0.1),\n            dict(type='MedianBlur', p=0.1),\n            dict(\n                type='CoarseDropout',\n                max_holes=1,\n                max_height=0.4,\n                max_width=0.4,\n                min_holes=1,\n                min_height=0.2,\n                min_width=0.2,\n                p=0.5),\n        ]),\n    dict(type='GenerateTarget', encoder=codec),\n    dict(type='PackPoseInputs')\n]\n\n# data loaders\ntrain_dataloader = dict(\n    batch_size=train_batch_size,\n    num_workers=2,\n    persistent_workers=True,\n    sampler=dict(type='DefaultSampler', shuffle=True),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=dataset_info,\n        data_mode=data_mode,\n        ann_file='trainval.json',\n        data_prefix=dict(img='images/'),\n        pipeline=train_pipeline,\n    ))\nval_dataloader = dict(\n    batch_size=val_batch_size,\n    num_workers=2,\n    persistent_workers=True,\n    drop_last=False,\n    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),\n    dataset=dict(\n        type=dataset_type,\n        data_root=data_root,\n        metainfo=dataset_info,\n        data_mode=data_mode,\n        ann_file='test.json',\n        data_prefix=dict(img='images/'),\n        pipeline=val_pipeline,\n    ))\ntest_dataloader = val_dataloader\n\ndefault_hooks = {\n    'checkpoint': {'save_best': 'PCK','rule': 'greater','max_keep_ckpts': 2},\n    'logger': {'interval': 50}\n}\n\ncustom_hooks = [\n    dict(\n        type='EMAHook',\n        ema_type='ExpMomentumEMA',\n        momentum=0.0002,\n        update_buffers=True,\n        priority=49),\n    dict(\n        type='mmdet.PipelineSwitchHook',\n        switch_epoch=max_epochs - stage2_num_epochs,\n        switch_pipeline=train_pipeline_stage2)\n]\n\n# evaluators\nval_evaluator = [\n    dict(type='CocoMetric', ann_file=data_root + 'test.json'),\n    dict(type='PCKAccuracy'),\n    dict(type='AUC'),\n    dict(type='NME', norm_mode='keypoint_distance', keypoint_indices=[0, 1])\n]\n\ntest_evaluator = val_evaluator\n\"\"\"\nconfig = './configs/reba_keypoint.py'\nwith io.open(config, 'w', encoding='utf-8') as f:\n    f.write(custom_config)","metadata":{},"execution_count":null,"outputs":[]}]}